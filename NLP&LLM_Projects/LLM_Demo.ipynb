{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Large Language Model Demonstration for NLP Tasks**"
      ],
      "metadata": {
        "id": "vd0XUveMmT9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The objective of the rest of this notebook will be to demonstrate certain key categories of NLP problems**, that can be solved to a good level by merely importing a Large Language Model and directly obtaining a prediction from it, without even needing to fine-tune the model on a specific corpus. This is, of course, merely a starting point for your hands-on implementation of Transformer-based architectures, and understanding the mathematical architecture behind these models and how that affects their potential areas of application will be the focus of the material moving forward."
      ],
      "metadata": {
        "id": "pm14aSDQqpXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **NLP Problem Categories for LLMs**"
      ],
      "metadata": {
        "id": "vbopL0MBrfJX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/wissaljawad/Projects/main/images/nlptasks.jpg\" alt=\"stoicbot\" width=\"500\" height=\"400\">"
      ],
      "metadata": {
        "id": "GJpx4XxQrAk5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now demonstrate a few examples that showcase the capabilities of Large Language Models."
      ],
      "metadata": {
        "id": "tiYArkcFJITm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Text Completion**\n",
        "\n",
        "<img src=\"https://cdn.pixabay.com/photo/2016/11/23/18/14/fountain-pen-1854169_960_720.jpg\" width=700px>\n",
        "\n",
        "We will create a simple function that takes a sentence or phrase as input, and uses the language model to generate the next word or words to complete the sentence. The model can generate text that is grammatically correct and contextually relevant."
      ],
      "metadata": {
        "id": "HtwJEPuyJUJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "s6dItT9LLoXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2J0oxVhcwmnp",
        "outputId": "0b4f32af-829a-40f5-83fd-04cb07ea1499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The quick brown fox is the most unique and colorful fox in the forest. It is a beautiful tree\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sat on the bed, the dog cuddled closer, the cat ate, the dog's\n",
            "When I woke up this morning, I was excited to find all my favorite films around. And I\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "text_completion = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "prompt = \"The quick brown fox\"\n",
        "completed_text = text_completion(prompt, max_length=20, do_sample=True)[0]['generated_text']\n",
        "print(completed_text)\n",
        "\n",
        "prompt = \"The cat sat on the\"\n",
        "completed_text = text_completion(prompt, max_length=20, do_sample=True)[0]['generated_text']\n",
        "print(completed_text)\n",
        "\n",
        "prompt = \"When I woke up this morning\"\n",
        "completed_text = text_completion(prompt, max_length=20, do_sample=True)[0]['generated_text']\n",
        "print(completed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Text Classification**\n",
        "\n",
        "<img src=\"https://cdn.pixabay.com/photo/2016/01/05/17/49/good-1123013_960_720.jpg\" width=700px>\n",
        "\n",
        "LLMs can be used for text classification tasks such as sentiment analysis, topic modeling, and spam filtering.\n",
        "\n",
        "Here's an example of sentiment analysis using the Hugging Face Transformers library."
      ],
      "metadata": {
        "id": "sG_N_U7gLxdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the sentiment analysis model\n",
        "model = pipeline('sentiment-analysis')\n",
        "\n",
        "# Analyze the sentiment of a text\n",
        "text = \"I love this product!\"\n",
        "result = model(text)[0]\n",
        "\n",
        "# Print the sentiment label and score\n",
        "print(f\"Label: {result['label']}, Score: {result['score']}\")\n",
        "\n",
        "# Analyze the sentiment of a text\n",
        "text = \"The plot was good but the characters were poorly developed.\"\n",
        "result = model(text)[0]\n",
        "\n",
        "# Print the sentiment label and score\n",
        "print(f\"Label: {result['label']}, Score: {result['score']}\")\n",
        "# Analyze the sentiment of a text\n",
        "text = \"The writing was brilliant, but the pacing was slow and dragged the story down.\"\n",
        "result = model(text)[0]\n",
        "\n",
        "# Print the sentiment label and score\n",
        "print(f\"Label: {result['label']}, Score: {result['score']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-MFMkvFOtJU",
        "outputId": "60fcf018-d665-4210-c8c4-dc75413557be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label: POSITIVE, Score: 0.9998855590820312\n",
            "Label: NEGATIVE, Score: 0.9994469285011292\n",
            "Label: NEGATIVE, Score: 0.9994577765464783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the pipeline method to load a pre-trained sentiment analysis model, and then applies the model to a sample text input. The result is a sentiment label (positive or negative) and a score indicating the confidence level of the model's prediction."
      ],
      "metadata": {
        "id": "mBqNVBvzO2rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Question Answering**\n",
        "\n",
        "<img src=\"https://cdn.pixabay.com/photo/2018/06/12/15/08/question-mark-3470783_960_720.jpg\" width=700px>\n",
        "\n",
        "LLMs can be used to answer questions based on a given text, which is of course an important part of Conversational AI experiences.\n",
        "\n",
        "Here's an example of question answering using the Hugging Face Transformers library."
      ],
      "metadata": {
        "id": "rasA-UWDO-Ln"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the question answering model\n",
        "model = pipeline('question-answering')\n",
        "\n",
        "# Answer a question based on a text\n",
        "text = \"The Mona Lisa is a 16th century portrait painted by Leonardo da Vinci.\"\n",
        "question = \"Who painted the Mona Lisa?\"\n",
        "result = model(question=question, context=text)\n",
        "\n",
        "# Print the answer\n",
        "print(result['answer'])\n",
        "\n",
        "# Answer a question based on a text\n",
        "text = \"The band members played their favourite songs for the audience during \\\n",
        "the concert last night. Despite the power outage, the band managed to complete \\\n",
        "their performance.\"\n",
        "question = \"What did the band do during the concert?\"\n",
        "result = model(question=question, context=text)\n",
        "\n",
        "# Print the answer\n",
        "print(result['answer'])\n",
        "\n",
        "# Answer a question based on a text\n",
        "text = \"The genetic makeup of a living organism is determined by its DNA, which \\\n",
        "is made up of nucleotides arranged in a specific sequence. This sequence \\\n",
        "provides instructions for the organism's physical and functional characteristics.\"\n",
        "question = \"What is the function of an organism's DNA?\"\n",
        "result = model(question=question, context=text)\n",
        "\n",
        "# Print the answer\n",
        "print(result['answer'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JMwkTkTuPClP",
        "outputId": "9019a197-ca9f-4c97-f94e-e698c8f32247"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Leonardo da Vinci\n",
            "played their favourite songs for the audience\n",
            "physical and functional characteristics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the pipeline method to load a pre-trained question answering model, and then applies the model to a given question and text input. The result is the answer to the question as a string."
      ],
      "metadata": {
        "id": "CDByj2n9PHyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Text Generation**\n",
        "\n",
        "<img src=\"https://cdn.pixabay.com/photo/2015/07/17/22/43/student-849825_960_720.jpg\" width=700px>\n",
        "\n",
        "\n",
        "LLMs can also be used for Text Generation - to generate longer paragraphs of text based on a given prompt (as opposed to just Text Completion which may be for a shorter expected output).\n",
        "\n",
        "Here's an example of text generation using the GPT-2 model and the Hugging Face Transformers library."
      ],
      "metadata": {
        "id": "Wtvbo3_ePN0m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load the GPT-2 tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "\n",
        "# Set the text prompt\n",
        "prompt = \"Once upon a time,\"\n",
        "\n",
        "# Generate text based on the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=50)\n",
        "\n",
        "# Decode the generated text and print it\n",
        "generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n",
        "# Set the text prompt\n",
        "prompt = \"The airplane was flying at 30,000 feet when suddenly it\"\n",
        "\n",
        "# Generate text based on the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=50)\n",
        "\n",
        "# Decode the generated text and print it\n",
        "generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n",
        "# Set the text prompt\n",
        "prompt = \"The cat slept on the sofa for two hours and then suddenly woke up \\\n",
        "and jumped on the\"\n",
        "\n",
        "# Generate text based on the prompt\n",
        "input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "sample_output = model.generate(input_ids, do_sample=True, max_length=50)\n",
        "\n",
        "# Decode the generated text and print it\n",
        "generated_text = tokenizer.decode(sample_output[0], skip_special_tokens=True)\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1jVAzDfPSFw",
        "outputId": "fe6de514-0669-4fb6-bb34-5776595c7297"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time, the enemy who had the most advantage over the first might even have lost all their victories.\n",
            "\n",
            "However the other men of the company were rather more successful than the weaker than the strong.\n",
            "\n",
            "\"You really thought you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The airplane was flying at 30,000 feet when suddenly it lost altitude and crashed. The pilot and a passenger both died.\n",
            "\n",
            "A little more time and one more person was badly hurt. The plane crashed into a wooded area on the south\n",
            "The cat slept on the sofa for two hours and then suddenly woke up and jumped on the sofa. A few minutes later, he ran into my window (which is really not my house). He ran in and grabbed the house's door but ran outside\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the GPT-2 tokenizer and model to generate text based on a given prompt.\n",
        "\n",
        "The generate method of the model is used to generate a sequence of tokens based on the input prompt, and the decode method of the tokenizer is used to convert the generated tokens back into readable text."
      ],
      "metadata": {
        "id": "wIlYg1dQPb-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. Text Summarization**\n",
        "\n",
        "<img src=\"https://cdn.pixabay.com/photo/2017/10/24/21/25/arrow-2886223_960_720.jpg\" width=700px>\n",
        "\n",
        "\n",
        "LLMs can also be used to summarize longer texts into shorter ones - a key use case of NLP that is common to various industries and businesses.\n",
        "\n",
        "Here's an example of text summarization using the T5 model and the Hugging Face Transformers library."
      ],
      "metadata": {
        "id": "1KUD_mYqPhPm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the T5 summarization model\n",
        "model = pipeline('summarization', model='t5-small', tokenizer='t5-small')\n",
        "\n",
        "# Summarize a longer text\n",
        "text = \"The rise of artificial intelligence is set to transform our world in ways \\\n",
        " we can barely imagine. AI will revolutionize the way we work, the way we learn, and \\\n",
        " the way we live. With the power of AI, we will be able to solve some of the world's \\\n",
        " most pressing problems, from climate change to poverty to disease.\"\n",
        "\n",
        "summary = model(text, max_length=50, min_length=10, do_sample=False)\n",
        "\n",
        "# Print the summarized text\n",
        "print(summary[0]['summary_text'])\n",
        "\n",
        "# Summarize a longer text\n",
        "text = \"The theory of relativity, proposed by Albert Einstein, revolutionized \\\n",
        "our understanding of space, time, and gravity. It has become one of the most \\\n",
        "famous scientific theories of all time, and has been the subject of countless \\\n",
        "studies and debates. Despite its wide acceptance and numerous applications, \\\n",
        "there are still many aspects of the theory that remain unresolved or \\\n",
        "unexplained, leading some scientists to propose new theories or modifications \\\n",
        "to the original theory.\"\n",
        "\n",
        "summary = model(text, max_length=50, min_length=10, do_sample=False)\n",
        "\n",
        "# Print the summarized text\n",
        "print(summary[0]['summary_text'])\n",
        "\n",
        "# Summarize a longer text\n",
        "text = \"The novel 'To Kill a Mockingbird' by Harper Lee is a classic of \\\n",
        "American literature. It is set in the 1930s in a small town in Alabama and \\\n",
        "tells the story of a young girl named Scout Finch and her family. The novel \\\n",
        "explores themes of racism, injustice, and social inequality, and has been \\\n",
        "lauded for its honest and moving portrayal of these issues. It has won numerous \\\n",
        "awards and has been adapted into multiple films and plays.\"\n",
        "\n",
        "summary = model(text, max_length=50, min_length=10, do_sample=False)\n",
        "\n",
        "# Print the summarized text\n",
        "print(summary[0]['summary_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo6txTIAPlB-",
        "outputId": "7f3f6c97-0287-40a2-8cce-43379e3b40d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the rise of artificial intelligence is set to transform our world in ways we can barely imagine . we will be able to solve some of the world's most pressing problems, from climate change to poverty .\n",
            "the theory of relativity, proposed by Albert Einstein, revolutionized our understanding of space, time, and gravity . there are still many aspects of the theory that remain unresolved or unexplained .\n",
            "the novel explores themes of racism, injustice, and social inequality . it has been lauded for its honest portrayal of these issues .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the `pipeline` method to load a pre-trained T5 summarization model, and then applies the model to a longer input text.\n",
        "\n",
        "The `max_length` and `min_length` parameters control the length of the summary output, and the `do_sample` parameter is set to `False` to ensure deterministic output."
      ],
      "metadata": {
        "id": "dPwgYMYIQAAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Named Entity Recognition (NER)**\n",
        "\n",
        "<img src=\"https://cdn.pixabay.com/photo/2017/10/14/01/37/feedback-2849601_960_720.jpg\" width=700px>\n",
        "\n",
        "Large language models can also be used for NER - to identify and extract named entities from text, such as person names, locations, and organizations.\n",
        "\n",
        "Here's an example of NER using the Hugging Face Transformers library:"
      ],
      "metadata": {
        "id": "C9i6PBwVQE1f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the named entity recognition model\n",
        "model = pipeline('ner', grouped_entities=True)\n",
        "\n",
        "# Identify named entities in a text\n",
        "text = \"Elon Musk is the CEO of Tesla and SpaceX, and lives in California.\"\n",
        "entities = model(text)\n",
        "\n",
        "# Print the named entities and their labels\n",
        "for entity in entities:\n",
        "    print(f\"{entity['word']}: {entity['entity_group']}\")\n",
        "\n",
        "# Identify named entities in a text\n",
        "text = \"I recently read a book called 'The Count of Monte Cristo' by Alexandre \\\n",
        "Dumas. The main character, Edmond Dantès, was imprisoned in Château d'If for a \\\n",
        "crime he didn't commit. He escaped with the help of a fellow prisoner named Abbé Faria.\"\n",
        "entities = model(text)\n",
        "\n",
        "# Print the named entities and their labels\n",
        "for entity in entities:\n",
        "    print(f\"{entity['word']}: {entity['entity_group']}\")\n",
        "\n",
        "# Identify named entities in a text\n",
        "text = \"I live in the city of Melbourne and work for a company called Apple. My \\\n",
        "boss is Tim Cook and my colleague's name is John Smith. Yesterday, I had lunch \\\n",
        "at a restaurant called San Francisco in the city center.\"\n",
        "entities = model(text)\n",
        "\n",
        "# Print the named entities and their labels\n",
        "for entity in entities:\n",
        "    print(f\"{entity['word']}: {entity['entity_group']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r2nFneTQN-a",
        "outputId": "b35402f3-ba6e-4032-cad4-caa8191ec738"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk: PER\n",
            "Tesla: ORG\n",
            "SpaceX: ORG\n",
            "California: LOC\n",
            "The Count of Monte Cristo: MISC\n",
            "Alexandre Dumas: PER\n",
            "Edmond Dantès: PER\n",
            "Château d ' If: LOC\n",
            "A: PER\n",
            "##é Faria: PER\n",
            "Melbourne: LOC\n",
            "Apple: ORG\n",
            "Tim Cook: PER\n",
            "John Smith: PER\n",
            "San Francisco: LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the pipeline method to load a pre-trained named entity recognition model, and then applies the model to a sample input text. The grouped_entities parameter is set to True to group related entities together, such as multiple locations or organizations."
      ],
      "metadata": {
        "id": "ES_WIna2QWB0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **7. Language Translation**\n",
        "\n",
        "<img src=\"https://cdn.pixabay.com/photo/2022/08/16/06/30/different-language-7389469_960_720.jpg\" width=700px>\n",
        "\n",
        "Another long-standing use case of NLP that is being solved by LLMs is Language Translation.\n",
        "\n",
        "Here's an example of translation from English to French and German using the Hugging Face Transformers library."
      ],
      "metadata": {
        "id": "p8QXT7fSSIFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the translation model\n",
        "model = pipeline('translation_en_to_fr')\n",
        "\n",
        "# Translate an English text to French\n",
        "text = \"The quick brown fox jumped over the lazy dog.\"\n",
        "translation = model(text, max_length=40)\n",
        "\n",
        "# Print the translated text\n",
        "print(translation[0]['translation_text'])\n",
        "\n",
        "# Load the translation model\n",
        "model = pipeline('translation_en_to_de')\n",
        "\n",
        "# Translate an English text to German\n",
        "text = \"The sun is shining and the birds are chirping\"\n",
        "translation = model(text, max_length=40)\n",
        "\n",
        "# Print the translated text\n",
        "print(translation[0]['translation_text'])\n",
        "\n",
        "# Load the translation model\n",
        "model = pipeline('translation_en_to_fr')\n",
        "\n",
        "# Translate an English text to French\n",
        "text = \"The blackboard is black\"\n",
        "translation = model(text, max_length=40)\n",
        "\n",
        "# Print the translated text\n",
        "print(translation[0]['translation_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8SBv1ZT6nx1",
        "outputId": "6cc2689d-15e8-47b6-ee55-43f1a779a59c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le renard brun rapide a sauté au-dessus du chien laxiste.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Die Sonne scheint und die Vögel chirren\n",
            "Le tableau noir est noir.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code uses the pipeline method to load a pre-trained language translation model, and then applies the model to an English input text.\n",
        "\n",
        "The max_length parameter controls the maximum length of the translated text output."
      ],
      "metadata": {
        "id": "TJh22YSiSPI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Conclusion**\n",
        "In conclusion, transformer-based LLMs have brought about a revolution in NLP and have opened up a world of possibilities for language-based applications. They are capable of performing a wide range of tasks as observed, and have greatly improved the accuracy and effectiveness of language-based applications, making them more user-friendly and efficient. Moreover open-source libraries such as Hugging Face have made these models easily accessible to developers, researchers, and data scientists, allowing them to create and fine-tune their own models.vAs these models continue to evolve and become more sophisticated, we can expect to see even more innovative and groundbreaking applications of NLP through LLMs in the future.\n",
        "\n",
        "However - it is important to point out that while many such LLMs are available and can directly be imported and used, **they are usually trained on general corpora of text, and can fall short of the specificity and quality expected from individual tasks.** As such, while using off-the-shelf LLMs is a good starting point, **it is also important while using Deep Learning for NLP to understand the architecture of these models and how they can be fine-tuned to further improve their quality**. This will be the main focus of our exploration of the world of Transformer-based models moving forward."
      ],
      "metadata": {
        "id": "wO4PfLjoUo6u"
      }
    }
  ]
}